{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  EXAMPLE delete this to run the code\n",
    "\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url = 'http://localhost:11434/v1',\n",
    "#     api_key='ollama', # requiAred, but unused\n",
    "# )\n",
    "\n",
    "\n",
    "# # Example with custom input_text\n",
    "\n",
    "\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List, Dict\n",
    "\n",
    "\n",
    "# class Thought(BaseModel):\n",
    "#         quoted_chunk: str = Field(..., description=\"the quoted unit of thought\")\n",
    "#         thought_classification: str= Field(..., description=\"the topic classification for the thought\")\n",
    "#         thought_classification_explanation: str= Field(..., description=\"explantion behind the classification\")\n",
    "#         thought_sentiment:  int = Field(..., description=\"the rated sentiment of this thought on 5 point scale 1 very negative, 2 is negative, 3 neutral, 4 positive, 5 very positive\")\n",
    "#         positivity: bool = Field(..., description= \"True = Positive, False=Negative\")\n",
    "#         thought_sentiment_explanation: str = Field(..., description=\"Sentiment rating 5 point scale, no decimals (1=very negative, 3 neutral, 5=very positive)\")\n",
    "        \n",
    "\n",
    "# class InvestmentPostTopicClassification(BaseModel):\n",
    "#     topic_classification: str = Field(..., description=\"Primary investment-related topic\")\n",
    "#     topic_explanation: str = Field(..., description=\"Explanation for the assigned topic\")\n",
    "#     sentiment_score: int = Field(..., description=\"Sentiment rating 5 point scale, no decimals (1=very negative, 3 neutral, 5=very positive)\")\n",
    "#     sentiment_explanation: str = Field(..., description=\"Explanation for the sentiment score\")\n",
    "#     thoughts: List[Thought]\n",
    "\n",
    "\n",
    "    \n",
    "# template = \"you are a expert at classifying topics and rating sentiments on a 5 point scale ( 1 very negative, 2 is negative, 3 neutral, 4 positive, 5 very positive) for crypto curriency posts on reddit. the text into units of thought you have been given the following post to ive a classification and sentiment:\"\n",
    "# input_text = \"doge and bitcoin were basically a scam on the general public.\"\n",
    "\n",
    "# json_completion = client.beta.chat.completions.parse(\n",
    "#         model=\"mistral:7b-instruct-v0.3-q4_0\",\n",
    "#         # response_format= { \"type\": \"json_schema\",\"json_schema\": json_struct},\n",
    "#         response_format= InvestmentPostTopicClassification,\n",
    "#         messages=[\n",
    "#                 {\"role\": \"user\", \"content\": template},\n",
    "#                 # {\"role\": \"user\", \"content\": },            \n",
    "#                 {\"role\": \"user\", \"content\": input_text},\n",
    "#         ],\n",
    "#         temperature=0.4,\n",
    "#         )\n",
    "# InvestmentPostTopicClassification.model_validate(json_completion.choices[0].message.parsed)\n",
    "\n",
    "# json_completion.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def retry_until_valid(func):\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         attempt = 1\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 return func(*args, **kwargs)\n",
    "#             except ValidationError as e:\n",
    "#                 print(f\"Attempt #{attempt} failed with validation error: {e}\")\n",
    "#                 attempt += 1\n",
    "#     return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example PSO with reddit scrape\n",
    "\n",
    "\n",
    "# import praw\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List, Dict\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "\n",
    "# import ollama\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url = 'http://localhost:11434/v1',\n",
    "#     api_key='ollama', # requiAred, but unused\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# # Reddit API setup\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id=os.getenv(\"REDDIT_CLIENT_ID\"),  # Replace with your Reddit API client ID\n",
    "#     client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),  # Replace with your Reddit API client secret\n",
    "#     user_agent=\"script:meme_coin_predictor:v1.0 (by u/Big_Seesaw_5202)\"  # Replace with your user agent, e.g., \"sentiment_analysis_bot:v1.0\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Function to scrape one Reddit post\n",
    "# def scrape_reddit_post(subreddit_name=\"CryptoCurrency\", limit=10):\n",
    "#     try:\n",
    "#         subreddit = reddit.subreddit(subreddit_name)\n",
    "#         for post in subreddit.new(limit=limit):\n",
    "#             if not post.stickied:  # Skip stickied posts\n",
    "#                 # Use selftext if available, otherwise fall back to title\n",
    "#                 text_content = post.selftext if post.selftext else post.title\n",
    "#                 if text_content.strip():  # Ensure there's non-empty text\n",
    "#                     return {\n",
    "#                         \"title\": post.title,\n",
    "#                         \"text\": text_content,\n",
    "#                         \"id\": post.id,\n",
    "#                         \"url\": post.url\n",
    "#                     }\n",
    "#         print(f\"No suitable posts found in r/{subreddit_name} with text content after checking {limit} posts.\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error scraping Reddit: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# # Pydantic models (unchanged)\n",
    "# class Thought(BaseModel):\n",
    "#     quoted_chunk: str = Field(..., description=\"the quoted unit of thought\")\n",
    "#     thought_classification: str = Field(..., description=\"the topic classification for the thought\")\n",
    "#     thought_classification_explanation: str = Field(..., description=\"explanation behind the classification\")\n",
    "#     thought_sentiment: int = Field(..., description=\"the rated sentiment of this thought on 5 point scale 1 very negative, 2 is negative, 3 neutral, 4 positive, 5 very positive\")\n",
    "#     positivity: bool = Field(..., description=\"True = Positive, False=Negative\")\n",
    "#     thought_sentiment_explanation: str = Field(..., description=\"Sentiment rating 5 point scale, no decimals (1=very negative, 3 neutral, 5=very positive)\")\n",
    "\n",
    "# class InvestmentPostTopicClassification(BaseModel):\n",
    "#     topic_classification: str = Field(..., description=\"Primary investment-related topic\")\n",
    "#     topic_explanation: str = Field(..., description=\"Explanation for the assigned topic\")\n",
    "#     sentiment_score: int = Field(..., description=\"Sentiment rating 5 point scale, no decimals (1=very negative, 3 neutral, 5=very positive)\")\n",
    "#     sentiment_explanation: str = Field(..., description=\"Explanation for the sentiment score\")\n",
    "#     thoughts: List[Thought]\n",
    "\n",
    "# # Template (unchanged)\n",
    "# template = \"you are a expert at classifying topics and rating sentiments on a 5 point scale ( 1 very negative, 2 is negative, 3 neutral, 4 positive, 5 very positive) for crypto curriency posts on reddit. the text into units of thought you have been given the following post to ive a classification and sentiment:\"\n",
    "\n",
    "# # Scrape one Reddit post\n",
    "# post = scrape_reddit_post()\n",
    "# if not post:\n",
    "#     raise Exception(\"No suitable Reddit post found with text content. Try a different subreddit or increase the limit.\")\n",
    "\n",
    "# # Use the post's text as input_text\n",
    "# input_text = post[\"text\"]\n",
    "# print(f\"Scraped Post ID: {post['id']}\")\n",
    "# print(f\"Post Title: {post['title']}\")\n",
    "# print(f\"Post Text: {input_text[:500]}...\")  # Print first 500 chars for reference\n",
    "\n",
    "# # LLM call\n",
    "# json_completion = client.beta.chat.completions.parse(\n",
    "#     model=\"gemma2:9b-instruct-q8_0\",\n",
    "#     response_format=InvestmentPostTopicClassification,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": template},\n",
    "#         {\"role\": \"user\", \"content\": input_text},\n",
    "#     ],\n",
    "#     temperature=0.4,\n",
    "# )\n",
    "\n",
    "# # Validate and process output\n",
    "# InvestmentPostTopicClassification.model_validate(json_completion.choices[0].message.parsed)\n",
    "\n",
    "# # Output the result\n",
    "# result = json_completion.to_dict()\n",
    "# print(\"LLM Output:\", result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# to run code, delete this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_response = InvestmentPostTopicClassification.model_validate_json(json_completion.choices[0].message.content)\n",
    "# json_completion.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dl1\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        return self.dinputs\n",
    "\n",
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "        else:\n",
    "            self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "            self.output = inputs * self.binary_mask\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        return self.dinputs\n",
    "\n",
    "class Layer_Input:\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        return self.dinputs\n",
    "\n",
    "class Activation_Linear:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        return self.dinputs\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0., clipnorm=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        self.clipnorm = clipnorm\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.momentum * layer.weight_cache - self.current_learning_rate * layer.dweights\n",
    "        layer.bias_cache = self.momentum * layer.bias_cache - self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        norm = np.sqrt(np.sum(layer.dweights**2) + np.sum(layer.dbiases**2))\n",
    "        if norm > self.clipnorm:\n",
    "            scale = self.clipnorm / (norm + 1e-8)\n",
    "            layer.dweights *= scale\n",
    "            layer.dbiases *= scale\n",
    "\n",
    "        layer.weights += layer.weight_cache\n",
    "        layer.biases += layer.bias_cache\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.clipnorm = clipnorm\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        # Initialize all necessary attributes\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        norm = np.sqrt(np.sum(layer.dweights**2) + np.sum(layer.dbiases**2))\n",
    "        if norm > self.clipnorm:\n",
    "            scale = self.clipnorm / (norm + 1e-8)\n",
    "            layer.dweights *= scale\n",
    "            layer.dbiases *= scale\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def regularization_loss(self):\n",
    "        regularization_loss = 0\n",
    "        for layer in self.trainable_layers:\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        return regularization_loss\n",
    "\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Ensure y_true has the same shape as y_pred\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Ensure y_true has the same shape as y_pred\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "        samples = len(y_pred)\n",
    "        # Compute gradient: dL/dy_pred = -2 * (y_true - y_pred) / n_samples\n",
    "        self.dinputs = -2 * (y_true - y_pred) / samples\n",
    "        return self.dinputs\n",
    "\n",
    "class Accuracy_Regression:\n",
    "    def __init__(self, tolerance=0.5):\n",
    "        self.tolerance = tolerance\n",
    "        self.new_pass()\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "    def calculate(self, predictions, y):\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        comparisons = np.abs(predictions - y) <= self.tolerance\n",
    "        accuracy = np.mean(comparisons)\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_accumulated(self):\n",
    "        if self.accumulated_count == 0:\n",
    "            return 0\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return min(accuracy, 1.0)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy is not None:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "        layer_count = len(self.layers)\n",
    "        self.trainable_layers = []\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        self.input_layer.forward(X, training)\n",
    "        output = self.input_layer.output\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, training)\n",
    "        return output\n",
    "\n",
    "    def backward(self, output, y):\n",
    "        dvalues = self.loss.backward(output, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            dvalues = layer.backward(dvalues)\n",
    "        return dvalues\n",
    "\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None):\n",
    "        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        train_steps = 1\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f'epoch: {epoch}')\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            for step in range(train_steps):\n",
    "                batch_X = X if batch_size is None else X[step * batch_size:(step + 1) * batch_size]\n",
    "                batch_y = y if batch_size is None else y[step * batch_size:(step + 1) * batch_size]\n",
    "                output = self.forward(batch_X, training=True)\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "                accuracy = self.accuracy.calculate(output, batch_y)\n",
    "                self.backward(output, batch_y)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, acc: {accuracy:.3f}, loss: {loss:.3f}, '\n",
    "                          f'(data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), '\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            self.history['train_loss'].append(epoch_loss)\n",
    "            self.history['train_acc'].append(epoch_accuracy)\n",
    "            print(f'training, acc: {epoch_accuracy:.3f}, loss: {epoch_loss:.3f}, '\n",
    "                  f'(data_loss: {epoch_data_loss:.3f}, reg_loss: {epoch_regularization_loss:.3f}), '\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size)\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_acc'].append(val_acc)\n",
    "            else:\n",
    "                self.history['val_loss'].append(None)\n",
    "                self.history['val_acc'].append(None)\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        validation_steps = 1\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        for step in range(validation_steps):\n",
    "            batch_X = X_val if batch_size is None else X_val[step * batch_size:(step + 1) * batch_size]\n",
    "            batch_y = y_val if batch_size is None else y_val[step * batch_size:(step + 1) * batch_size]\n",
    "            output = self.forward(batch_X, training=False)\n",
    "            self.loss.calculate(output, batch_y)\n",
    "            self.accuracy.calculate(output, batch_y)\n",
    "\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "        print(f'validation, acc: {validation_accuracy:.3f}, loss: {validation_loss:.3f}')\n",
    "        return validation_loss, validation_accuracy\n",
    "\n",
    "    def predict(self, X, *, batch_size=None):\n",
    "        prediction_steps = 1\n",
    "        if batch_size is not None:\n",
    "            prediction_steps = len(X) // batch_size\n",
    "            if prediction_steps * batch_size < len(X):\n",
    "                prediction_steps += 1\n",
    "        output = []\n",
    "        for step in range(prediction_steps):\n",
    "            batch_X = X if batch_size is None else X[step * batch_size:(step + 1) * batch_size]\n",
    "            batch_output = self.forward(batch_X, training=False)\n",
    "            output.append(batch_output)\n",
    "        return np.vstack(output)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the model to a file.\"\"\"\n",
    "        model_data = {\n",
    "            'layers': [],\n",
    "            'loss': self.loss,\n",
    "            'optimizer': self.optimizer,\n",
    "            'accuracy': self.accuracy,\n",
    "            'trainable_layers': []\n",
    "        }\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer_data = {\n",
    "                'class_name': layer.__class__.__name__,\n",
    "                'attributes': {}\n",
    "            }\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                layer_data['attributes'] = {\n",
    "                    'weights': layer.weights,\n",
    "                    'biases': layer.biases,\n",
    "                    'weight_regularizer_l1': layer.weight_regularizer_l1,\n",
    "                    'weight_regularizer_l2': layer.weight_regularizer_l2,\n",
    "                    'bias_regularizer_l1': layer.bias_regularizer_l1,\n",
    "                    'bias_regularizer_l2': layer.bias_regularizer_l2\n",
    "                }\n",
    "            elif isinstance(layer, Layer_Dropout):\n",
    "                layer_data['attributes'] = {\n",
    "                    'rate': layer.rate\n",
    "                }\n",
    "            elif isinstance(layer, (Activation_ReLU, Activation_Linear)):\n",
    "                layer_data['attributes'] = {}\n",
    "            model_data['layers'].append(layer_data)\n",
    "\n",
    "        # Save trainable layers references (indices)\n",
    "        for layer in self.trainable_layers:\n",
    "            layer_idx = next(i for i, l in enumerate(self.layers) if l is layer)\n",
    "            model_data['trainable_layers'].append(layer_idx)\n",
    "\n",
    "        # Save the model data\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        \"\"\"Load a model from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        model = Model()\n",
    "        model.layers = []\n",
    "        layer_map = {\n",
    "            'Layer_Dense': Layer_Dense,\n",
    "            'Layer_Dropout': Layer_Dropout,\n",
    "            'Activation_ReLU': Activation_ReLU,\n",
    "            'Activation_Linear': Activation_Linear\n",
    "        }\n",
    "\n",
    "        # Reconstruct layers\n",
    "        for layer_data in model_data['layers']:\n",
    "            class_name = layer_data['class_name']\n",
    "            attrs = layer_data['attributes']\n",
    "            if class_name == 'Layer_Dense':\n",
    "                # Create a temporary layer to get the shape\n",
    "                temp_layer = Layer_Dense(1, 1)  # Dummy initialization\n",
    "                layer = Layer_Dense(\n",
    "                    n_inputs=attrs['weights'].shape[0],\n",
    "                    n_neurons=attrs['weights'].shape[1],\n",
    "                    weight_regularizer_l1=attrs['weight_regularizer_l1'],\n",
    "                    weight_regularizer_l2=attrs['weight_regularizer_l2'],\n",
    "                    bias_regularizer_l1=attrs['bias_regularizer_l1'],\n",
    "                    bias_regularizer_l2=attrs['bias_regularizer_l2']\n",
    "                )\n",
    "                layer.weights = attrs['weights']\n",
    "                layer.biases = attrs['biases']\n",
    "            elif class_name == 'Layer_Dropout':\n",
    "                layer = Layer_Dropout(rate=attrs['rate'])\n",
    "            elif class_name in ['Activation_ReLU', 'Activation_Linear']:\n",
    "                layer = layer_map[class_name]()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type: {class_name}\")\n",
    "            model.add(layer)\n",
    "\n",
    "        # Set loss, optimizer, and accuracy\n",
    "        model.set(\n",
    "            loss=model_data['loss'],\n",
    "            optimizer=model_data['optimizer'],\n",
    "            accuracy=model_data['accuracy']\n",
    "        )\n",
    "\n",
    "        # Rebuild trainable layers list\n",
    "        model.trainable_layers = [model.layers[idx] for idx in model_data['trainable_layers']]\n",
    "\n",
    "        # Finalize the model\n",
    "        model.finalize()\n",
    "\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_______________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#remove ollama summary generation for speed testing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pydantic models (unchanged)\n",
    "class Thought(BaseModel):\n",
    "    quoted_chunk: str = Field(..., description=\"the quoted unit of thought\")\n",
    "    thought_classification: str = Field(..., description=\"the topic classification for the thought\")\n",
    "    thought_classification_explanation: str = Field(..., description=\"explanation behind the classification\")\n",
    "    thought_sentiment: int = Field(..., description=\"the rated sentiment of this thought on 5 point scale 1 very negative, 2 is negative, 3 neutral, 4 positive, 5 very positive\")\n",
    "    positivity: bool = Field(..., description=\"True = Positive, False=Negative\")\n",
    "    thought_sentiment_explanation: str = Field(..., description=\"Sentiment rating 5 point scale, no decimals (1=very negative, 3 neutral, 5=very positive)\")\n",
    "\n",
    "class InvestmentPostTopicClassification(BaseModel):\n",
    "    topic_classification: str = Field(..., description=\"Primary investment-related topic\")\n",
    "    topic_explanation: str = Field(..., description=\"Explanation for the assigned topic\")\n",
    "    sentiment_score: int = Field(..., description=\"Sentiment rating 5 point scale, no decimals (1=very negative, 3 neutral, 5=very positive)\")\n",
    "    sentiment_explanation: str = Field(..., description=\"Explanation for the sentiment score\")\n",
    "    thoughts: List[Thought]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API setup\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=\"script:meme_coin_predictor:v1.0 (by u/Big_Seesaw_5202)\"\n",
    ")\n",
    "\n",
    "# --- Scraping Section ---\n",
    "def scrape_reddit_posts(subreddits=None, posts_per_subreddit=100, existing_ids=None):\n",
    "    if subreddits is None:\n",
    "        subreddits = [\"CryptoCurrency\", \"CryptoMoonShots\", \"SatoshiStreetBets\", \"CryptoMarkets\", \"altcoin\"]\n",
    "    if existing_ids is None:\n",
    "        existing_ids = set()\n",
    "    posts = []\n",
    "    for subreddit_name in subreddits:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            post_count = 0\n",
    "            for post in subreddit.new(limit=posts_per_subreddit * 2):  # Overshoot to account for skips\n",
    "                if post.stickied or post.is_self is False:\n",
    "                    continue\n",
    "                if post.id in existing_ids:\n",
    "                    continue\n",
    "                text_content = post.selftext if post.selftext else post.title\n",
    "                if not text_content.strip():\n",
    "                    continue\n",
    "                posts.append({\n",
    "                    \"id\": post.id,\n",
    "                    \"title\": post.title,\n",
    "                    \"text\": text_content,\n",
    "                    \"url\": post.url,\n",
    "                    \"subreddit\": subreddit_name\n",
    "                })\n",
    "                post_count += 1\n",
    "                if post_count >= posts_per_subreddit:\n",
    "                    break\n",
    "            print(f\"Scraped {post_count} new posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping r/{subreddit_name}: {str(e)}\")\n",
    "    print(f\"Total new posts scraped: {len(posts)}\")\n",
    "    return posts\n",
    "\n",
    "# --- Input Text Processing Section ---\n",
    "def get_ollama_embedding(text, model='nomic-embed-text'):\n",
    "    response = ollama.embeddings(model=model, prompt=text)\n",
    "    return np.array(response['embedding'])\n",
    "\n",
    "def process_reddit_posts(posts, cache_file=\"reddit_embeddings.pkl\"):\n",
    "    # Load existing cache if it exists\n",
    "    existing_data = {'ids': [], 'X': np.array([]), 'y': np.array([]), 'df': pd.DataFrame()}\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            existing_data = pickle.load(f)\n",
    "        print(f\"Loaded existing cache from {cache_file} with {len(existing_data['ids'])} posts\")\n",
    "\n",
    "    existing_ids = set(existing_data['ids'])\n",
    "    new_posts = [post for post in posts if post['id'] not in existing_ids]\n",
    "    if not new_posts:\n",
    "        print(\"No new posts to process, returning existing data\")\n",
    "        return np.array(existing_data['X']), np.array(existing_data['y']), existing_data['df']\n",
    "\n",
    "    # Initialize lists with existing data if available\n",
    "    X = list(existing_data['X']) if existing_data['X'].size > 0 else []\n",
    "    y = list(existing_data['y']) if existing_data['y'].size > 0 else []\n",
    "    texts = existing_data['df']['text'].tolist() if not existing_data['df'].empty else []\n",
    "    ids = existing_data['ids']\n",
    "    results = existing_data['df']['llm_result'].tolist() if not existing_data['df'].empty else []\n",
    "    subreddits = existing_data['df']['subreddit'].tolist() if not existing_data['df'].empty else []\n",
    "\n",
    "    template = \"you are a expert at classifying topics and rating sentiments on a 5 point scale ( 1 very negative, 2 is negative, 3 neutral, 4 positive, 5 very positive) for crypto currency posts on reddit. the text into units of thought you have been given the following post to ive a classification and sentiment:\"\n",
    "\n",
    "    for post in new_posts:\n",
    "        input_text = post['text']\n",
    "        try:\n",
    "            json_completion = client.beta.chat.completions.parse(\n",
    "                model=\"mistral:7b-instruct-v0.3-q4_0\",\n",
    "                response_format=InvestmentPostTopicClassification,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": template},\n",
    "                    {\"role\": \"user\", \"content\": input_text},\n",
    "                ],\n",
    "                temperature=0.4,\n",
    "            )\n",
    "            result = json_completion.choices[0].message.parsed\n",
    "            InvestmentPostTopicClassification.model_validate(result)\n",
    "            thought_sentiments = [thought.thought_sentiment for thought in result.thoughts]\n",
    "            avg_sentiment = np.mean(thought_sentiments) if thought_sentiments else result.sentiment_score\n",
    "            embedding = get_ollama_embedding(input_text)\n",
    "            X.append(embedding)\n",
    "            y.append(avg_sentiment)\n",
    "            texts.append(input_text)\n",
    "            ids.append(post['id'])\n",
    "            results.append(result.model_dump())\n",
    "            subreddits.append(post['subreddit'])\n",
    "            print(f\"\\nStructured Output for Post ID: {post['id']} (r/{post['subreddit']})\")\n",
    "            print(f\"Title: {post['title']}\")\n",
    "            print(json.dumps(result.model_dump(), indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post {post['id']}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Combine data into DataFrame\n",
    "    titles = [post['title'] for post in posts if post['id'] in ids]\n",
    "    if len(titles) != len(ids):\n",
    "        # Handle cases where titles might be missing for some IDs\n",
    "        titles.extend(['Unknown'] * (len(ids) - len(titles)))\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'subreddit': subreddits,\n",
    "        'title': titles,\n",
    "        'text': texts,\n",
    "        'sentiment_score': y,\n",
    "        'llm_result': results\n",
    "    })\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X) if X else np.array([])\n",
    "    y = np.array(y) if y else np.array([])\n",
    "\n",
    "    print(\"\\nProcessed Reddit posts:\")\n",
    "    print(df[['id', 'subreddit', 'title', 'sentiment_score']].to_string(index=False))\n",
    "\n",
    "    # Save updated cache\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({'ids': ids, 'X': X, 'y': y, 'df': df}, f)\n",
    "    print(f\"Updated cache saved to {cache_file}\")\n",
    "\n",
    "    return X, y, df\n",
    "\n",
    "    # Combine data into DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'subreddit': subreddits,\n",
    "        'title': [post['title'] for post in posts if post['id'] in ids] if new_posts else existing_data['df']['title'],\n",
    "        'text': texts,\n",
    "        'sentiment_score': y,\n",
    "        'llm_result': results\n",
    "    })\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X) if X else existing_data['X']\n",
    "    y = np.array(y) if y else existing_data['y']\n",
    "\n",
    "    print(\"\\nProcessed Reddit posts:\")\n",
    "    print(df[['id', 'subreddit', 'title', 'sentiment_score']].to_string(index=False))\n",
    "\n",
    "    # Save updated cache\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump({'ids': ids, 'X': X, 'y': y, 'df': df}, f)\n",
    "    print(f\"Updated cache saved to {cache_file}\")\n",
    "\n",
    "    return X, y, df\n",
    "\n",
    "def normalize_embeddings(X):\n",
    "    if X.size == 0:\n",
    "        return X\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0) + 1e-8\n",
    "    return (X - X_mean) / X_std\n",
    "\n",
    "def create_reddit_data(subreddits=None, posts_per_subreddit=100, existing_ids=None):\n",
    "    posts = scrape_reddit_posts(subreddits, posts_per_subreddit, existing_ids)\n",
    "    if not posts:\n",
    "        raise Exception(\"No new Reddit posts found.\")\n",
    "    train_posts, test_posts = train_test_split(posts, test_size=0.2, random_state=42)\n",
    "    X_train, y_train, df_train = process_reddit_posts(train_posts, cache_file=\"train_reddit_embeddings.pkl\")\n",
    "    X_test, y_test, df_test = process_reddit_posts(test_posts, cache_file=\"test_reddit_embeddings.pkl\")\n",
    "    X_train = normalize_embeddings(X_train)\n",
    "    X_test = normalize_embeddings(X_test)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    return X_train, y_train, X_test, y_test, df_train, df_test\n",
    "\n",
    "DATA_FILE = \"reddit_data.pkl\"\n",
    "\n",
    "def load_or_create_data(subreddits, posts_per_subreddit):\n",
    "    existing_data = {'X_train': np.array([]), 'y_train': np.array([]), 'X_test': np.array([]), 'y_test': np.array([]), 'df_train': pd.DataFrame(), 'df_test': pd.DataFrame()}\n",
    "    existing_ids = set()\n",
    "\n",
    "    # Load existing data if available\n",
    "    if os.path.exists(DATA_FILE):\n",
    "        print(f\"Loading existing data from {DATA_FILE}\")\n",
    "        with open(DATA_FILE, 'rb') as f:\n",
    "            existing_data = pickle.load(f)\n",
    "        # Collect all existing post IDs\n",
    "        existing_ids = set(existing_data['df_train']['id']).union(set(existing_data['df_test']['id']))\n",
    "        print(f\"Found {len(existing_ids)} existing posts\")\n",
    "\n",
    "    # Scrape new posts, excluding existing IDs\n",
    "    new_posts = scrape_reddit_posts(subreddits, posts_per_subreddit, existing_ids)\n",
    "    if not new_posts:\n",
    "        print(\"No new posts scraped, returning existing data\")\n",
    "        return (existing_data['X_train'], existing_data['y_train'], existing_data['X_test'], existing_data['y_test'], existing_data['df_train'], existing_data['df_test'])\n",
    "\n",
    "    # Process new posts\n",
    "    train_posts, test_posts = train_test_split(new_posts, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Load and update train cache\n",
    "    train_cache_file = \"train_reddit_embeddings.pkl\"\n",
    "    X_train_new, y_train_new, df_train_new = process_reddit_posts(train_posts, cache_file=train_cache_file)\n",
    "    \n",
    "    # Load and update test cache\n",
    "    test_cache_file = \"test_reddit_embeddings.pkl\"\n",
    "    X_test_new, y_test_new, df_test_new = process_reddit_posts(test_posts, cache_file=test_cache_file)\n",
    "\n",
    "    # Combine existing and new data\n",
    "    X_train = np.vstack([existing_data['X_train'], X_train_new]) if existing_data['X_train'].size else X_train_new\n",
    "    y_train = np.concatenate([existing_data['y_train'], y_train_new]) if existing_data['y_train'].size else y_train_new\n",
    "    X_test = np.vstack([existing_data['X_test'], X_test_new]) if existing_data['X_test'].size else X_test_new\n",
    "    y_test = np.concatenate([existing_data['y_test'], y_test_new]) if existing_data['y_test'].size else y_test_new\n",
    "    df_train = pd.concat([existing_data['df_train'], df_train_new], ignore_index=True)\n",
    "    df_test = pd.concat([existing_data['df_test'], df_test_new], ignore_index=True)\n",
    "\n",
    "    # Normalize combined data\n",
    "    X_train = normalize_embeddings(X_train)\n",
    "    X_test = normalize_embeddings(X_test)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    # Save updated data\n",
    "    data = {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'df_train': df_train, 'df_test': df_test\n",
    "    }\n",
    "    with open(DATA_FILE, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Updated data saved to {DATA_FILE}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, df_train, df_test\n",
    "\n",
    "def get_result_by_post_id(df, post_id):\n",
    "    result_row = df[df['id'] == post_id]\n",
    "    if result_row.empty:\n",
    "        print(f\"No post found with ID: {post_id}\")\n",
    "        return None\n",
    "    llm_result = result_row['llm_result'].iloc[0]\n",
    "    subreddit = result_row.get('subreddit', 'Unknown').iloc[0]\n",
    "    print(f\"\\nStructured Output for Post ID: {post_id} (r/{subreddit})\")\n",
    "    print(f\"Title: {result_row['title'].iloc[0]}\")\n",
    "    print(json.dumps(llm_result, indent=2))\n",
    "    return llm_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API setup\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=\"script:meme_coin_predictor:v1.0 (by u/Big_Seesaw_5202)\"\n",
    ")\n",
    "\n",
    "# Main: Scrape and cache\n",
    "subreddits = [\"CryptoCurrency\", \"CryptoMoonShots\", \"SatoshiStreetBets\", \"CryptoMarkets\", \"altcoin\"]\n",
    "posts_per_subreddit = 300\n",
    "\n",
    "existing_data = {'train_posts': [], 'test_posts': [], 'df_train': pd.DataFrame(), 'df_test': pd.DataFrame()}\n",
    "existing_ids = set()\n",
    "\n",
    "if os.path.exists(DATA_FILE):\n",
    "    print(f\"Loading existing data from {DATA_FILE}\")\n",
    "    with open(DATA_FILE, 'rb') as f:\n",
    "        existing_data = pickle.load(f)\n",
    "    existing_ids = set(existing_data['df_train']['id']).union(set(existing_data['df_test']['id']))\n",
    "    print(f\"Found {len(existing_ids)} existing posts\")\n",
    "\n",
    "new_posts = scrape_reddit_posts(subreddits, posts_per_subreddit, existing_ids)\n",
    "if new_posts:\n",
    "    train_posts, test_posts = train_test_split(new_posts, test_size=0.2, random_state=42)\n",
    "    existing_data['train_posts'].extend(train_posts)\n",
    "    existing_data['test_posts'].extend(test_posts)\n",
    "    \n",
    "    df_train = pd.DataFrame(existing_data['train_posts'])\n",
    "    df_test = pd.DataFrame(existing_data['test_posts'])\n",
    "    existing_data['df_train'] = df_train\n",
    "    existing_data['df_test'] = df_test\n",
    "\n",
    "    with open(DATA_FILE, 'wb') as f:\n",
    "        pickle.dump(existing_data, f)\n",
    "    print(f\"Updated raw post data saved to {DATA_FILE}\")\n",
    "else:\n",
    "    print(\"No new posts scraped, using existing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client for LLM\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "# Main: LLM sentiment scoring\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(f\"No raw post data found at {DATA_FILE}. Run the scraping cell first.\")\n",
    "else:\n",
    "    with open(DATA_FILE, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_posts = data['train_posts']\n",
    "    test_posts = data['test_posts']\n",
    "\n",
    "    train_cache_file = \"train_reddit_embeddings.pkl\"\n",
    "    X_train, y_train, df_train = process_reddit_posts(train_posts, cache_file=train_cache_file)\n",
    "\n",
    "    test_cache_file = \"test_reddit_embeddings.pkl\"\n",
    "    X_test, y_test, df_test = process_reddit_posts(test_posts, cache_file=test_cache_file)\n",
    "\n",
    "    X_train = normalize_embeddings(X_train)\n",
    "    X_test = normalize_embeddings(X_test)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    processed_data = {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'df_train': df_train, 'df_test': df_test\n",
    "    }\n",
    "    with open(DATA_FILE, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    print(f\"Updated processed data saved to {DATA_FILE}\")\n",
    "\n",
    "    print(\"Train sentiment mean:\", np.mean(y_train))\n",
    "    print(\"Test sentiment mean:\", np.mean(y_test))\n",
    "    print(f\"Total posts processed: {len(df_train) + len(df_test)}\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main: Train neural network and generate model\n",
    "learning_rate = 0.0005\n",
    "batch_size = 4\n",
    "l2_reg = 5e-3\n",
    "epochs = 100\n",
    "patience = 10\n",
    "\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(f\"No processed data found at {DATA_FILE}. Run the LLM scoring cell first.\")\n",
    "else:\n",
    "    with open(DATA_FILE, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train, y_train, X_test, y_test, df_train, df_test = (\n",
    "        data['X_train'], data['y_train'], data['X_test'], data['y_test'], data['df_train'], data['df_test']\n",
    "    )\n",
    "\n",
    "    if len(y_train.shape) > 1:\n",
    "        y_train = y_train.flatten()\n",
    "        y_test = y_test.flatten()\n",
    "        print(\"Reshaped y_train and y_test to 1D arrays\")\n",
    "\n",
    "    model = Model()\n",
    "    model.add(Layer_Dense(768, 64, weight_regularizer_l2=l2_reg))\n",
    "    model.add(Activation_ReLU())\n",
    "    model.add(Layer_Dropout(0.2))\n",
    "    model.add(Layer_Dense(64, 1, weight_regularizer_l2=l2_reg))\n",
    "    model.add(Activation_Linear())\n",
    "    model.set(\n",
    "        loss=Loss_MeanSquaredError(),\n",
    "        optimizer=Optimizer_Adam(learning_rate=learning_rate, decay=1e-5, clipnorm=0.5),\n",
    "        accuracy=Accuracy_Regression(tolerance=0.5)\n",
    "    )\n",
    "    model.finalize()\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train(X_train, y_train, epochs=1, batch_size=batch_size, print_every=1000, validation_data=(X_test, y_test))\n",
    "        train_loss = np.mean(model.loss.forward(model.forward(X_train, training=False), y_train))\n",
    "        val_loss = np.mean(model.loss.forward(model.forward(X_test, training=False), y_test))\n",
    "        train_acc = model.accuracy.calculate(model.forward(X_train, training=False), y_train)\n",
    "        val_acc = model.accuracy.calculate(model.forward(X_test, training=False), y_test)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        if np.isnan(train_loss) or np.isnan(val_loss):\n",
    "            print(\"NaN detected, stopping training\")\n",
    "            break\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            model.save(\"BabyLlama.pkl\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nBest Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    if os.path.exists(\"BabyLlama.pkl\"):\n",
    "        model = Model.load(\"BabyLlama.pkl\")\n",
    "        predictions = model.predict(X_test)\n",
    "        df_test['predicted_sentiment'] = predictions.flatten()\n",
    "\n",
    "        print(\"\\nTest DataFrame with Predictions:\")\n",
    "        print(df_test[['id Paket', 'subreddit', 'title', 'sentiment_score', 'predicted_sentiment']].to_string(index=False))\n",
    "\n",
    "        epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "        plt.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "        plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if not df_test.empty:\n",
    "        example_post_id = df_test['id'].iloc[0]\n",
    "        result_row = df_test[df_test['id'] == example_post_id]\n",
    "        if not result_row.empty:\n",
    "            llm_result = result_row['llm_result'].iloc[0]\n",
    "            subreddit = result_row.get('subreddit', 'Unknown').iloc[0]\n",
    "            print(f\"\\nStructured Output for Post ID: {example_post_id} (r/{subreddit})\")\n",
    "            print(f\"Title: {result_row['title'].iloc[0]}\")\n",
    "            print(json.dumps(llm_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data from reddit_data.pkl\n",
      "Found 96 existing posts\n",
      "Scraped 71 new posts from r/CryptoCurrency\n",
      "Scraped 150 new posts from r/CryptoMoonShots\n",
      "Scraped 150 new posts from r/SatoshiStreetBets\n",
      "Scraped 150 new posts from r/CryptoMarkets\n",
      "Scraped 150 new posts from r/altcoin\n",
      "Total new posts scraped: 671\n",
      "Loaded existing cache from train_reddit_embeddings.pkl with 76 posts\n",
      "\n",
      "Structured Output for Post ID: 1kftun1 (r/CryptoMarkets)\n",
      "Title: ETH is stuck because of L2 leverage, fake volume, and no real demand\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency Analysis\",\n",
      "  \"topic_explanation\": \"The post discusses the performance of Ethereum (ETH) and its layer 2 (L2) ecosystem, as well as the rise of Solana and Sui.\",\n",
      "  \"sentiment_score\": 1,\n",
      "  \"sentiment_explanation\": \"The sentiment in this post is negative. The author expresses disappointment about the lack of success of ETH's L2 solutions, weak usage, and low liquidity. They also mention the decline of NFTs and DeFi on Ethereum, and the loss of market dominance to Solana and Sui.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"ETH price isn\\u2019t moving\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that the price of ETH is not increasing.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"This thought expresses dissatisfaction with the current state of ETH's price.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"most of these L2s didn\\u2019t take off. Real usage is weak.\",\n",
      "      \"thought_classification\": \"Analysis\",\n",
      "      \"thought_classification_explanation\": \"The author analyzes the lack of success and low usage of Ethereum's layer 2 solutions.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"This thought expresses disappointment with the performance of ETH's L2 solutions.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The worst part? While this was happening, the ETH Foundation sold near local tops.\",\n",
      "      \"thought_classification\": \"Evaluation\",\n",
      "      \"thought_classification_explanation\": \"The author evaluates the decision of the ETH Foundation to sell near local tops.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"This thought expresses criticism towards the ETH Foundation for selling at what appears to be an unfavorable time.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"It\\u2019s now eating ETH\\u2019s lunch in active users, stablecoin velocity, and developer mindshare.\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that Solana is gaining traction in terms of active users, stablecoin velocity, and developer interest.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"This thought expresses disappointment with the decline of ETH's market position compared to Solana.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1jo10cq (r/altcoin)\n",
      "Title: GUNZ is the Future of Web3 Gaming\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a cryptocurrency named GUNZ, which is a high-performance Layer 1 blockchain developed for gaming. It is used within the game 'Off The Grid' and is also listed on Bitget.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The post is generally positive as it highlights the potential of GUNZ in powering AAA Web3 gaming, its scalability, ease of integration for game developers, and the upcoming listing on Bitget. The mention of an airdrop also adds to the positivity.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"GUNZ is a high-performance Layer 1 blockchain developed by Gunzilla Games to power AAA Web3 gaming.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the purpose and developer of GUNZ.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Originally built for 'Off The Grid' (OTG), a cyberpunk battle royale, it has evolved into a full blockchain gaming platform...\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides additional information about the history and evolution of GUNZ.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"In OTG, all in-game items are NFTs, granting players true ownership and the ability to trade assets in-game or on external marketplaces like OpenSea.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the use of NFTs within OTG and the trading possibilities.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The ecosystem is fueled by $GUN, which players use to buy rare items, weapons, and upgrades while earning rewards through gameplay.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the role of $GUN within the ecosystem.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"GUNZ is designed for speed, security, and scalability, supporting 900,000+ daily active users and millions of transactions.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the performance capabilities of GUNZ.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"It enables Web2 game developers to integrate blockchain features without full redevelopment, reducing risks and accelerating adoption.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the benefits of GUNZ for game developers.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The $GUN token is set to list on Bitget in a few hours.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the upcoming listing of $GUN on Bitget.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is just a factual statement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Users can deposit and complete simple tasks to earn a $10 - $20 $GUN airdrop.\",\n",
      "      \"thought_classification\": \"Information\",\n",
      "      \"thought_classification_explanation\": \"This statement provides information about the airdrop opportunity for users.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"Negative sentiment as it might be seen as an incentive to invest or participate in activities that could potentially carry risk.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"As GUNZ expands, it is redefining blockchain gaming with true asset ownership and seamless integration.\",\n",
      "      \"thought_classification\": \"Prediction\",\n",
      "      \"thought_classification_explanation\": \"This statement predicts the future impact of GUNZ on blockchain gaming.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"Neutral sentiment as it is a prediction. However, the positive connotation about redefining and seamless integration adds to the overall positivity of the post.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1khuww0 (r/CryptoCurrency)\n",
      "Title: Movement Labs' Modular Architecture: Bridging Chains for Interoperability and Liquidity\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency/Blockchain Development\",\n",
      "  \"topic_explanation\": \"The post discusses the Movement SDK, a tool designed for unifying the Move and EVM ecosystems to promote interoperability and liquidity across different chains. The main focus is on its role in simplifying blockchain integration, application deployment, and enhancing developer experience.\",\n",
      "  \"sentiment_score\": 5,\n",
      "  \"sentiment_explanation\": \"The post is overwhelmingly positive as it highlights the advantages of the Movement SDK, emphasizing its potential to improve security, performance, interoperability, and liquidity within the blockchain ecosystem.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"The Movement SDK is a key tool within Movement Labs' architecture, designed to unify the Move and EVM (Ethereum Virtual Machine) ecosystems\",\n",
      "      \"thought_classification\": \"Introduction/Background\",\n",
      "      \"thought_classification_explanation\": \"This sentence sets the stage for the post by introducing the Movement SDK and its purpose.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it introduces a useful tool designed to improve blockchain ecosystems.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"It's operationalized through M1, a decentralized sequencer network.\",\n",
      "      \"thought_classification\": \"Functionality/Operation\",\n",
      "      \"thought_classification_explanation\": \"This sentence explains how the Movement SDK is executed.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it highlights an important aspect of the Movement SDK's operation.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"*Bridging EVM and Move Ecosystems (Full Composability):* The SDK, particularly through the **MoveVM** and **Fractal**, allows for the first time total composability between the EVM and Move worlds.\",\n",
      "      \"thought_classification\": \"Advantages/Benefits\",\n",
      "      \"thought_classification_explanation\": \"This section discusses one of the main advantages offered by the Movement SDK: bridging EVM and Move ecosystems.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it highlights a significant advantage of the Movement SDK.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"*Enhanced Interoperability and Liquidity:* By connecting the EVM and Move ecosystems, the SDK fosters native interoperability and deep liquidity.\",\n",
      "      \"thought_classification\": \"Advantages/Benefits\",\n",
      "      \"thought_classification_explanation\": \"This section discusses another advantage of the Movement SDK: enhanced interoperability and liquidity.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it highlights a significant advantage of the Movement SDK.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"*Improved Developer Experience and Accessibility:* The SDK provides developers with the tools and infrastructure needed to build and deploy applications quickly and easily using the Move Stack.\",\n",
      "      \"thought_classification\": \"Advantages/Benefits\",\n",
      "      \"thought_classification_explanation\": \"This section discusses another advantage of the Movement SDK: improved developer experience.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it highlights a significant advantage of the Movement SDK for developers.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"*Leveraging Move's Inherent Security:* The SDK brings the security advantages of the Move language and MoveVM to the forefront.\",\n",
      "      \"thought_classification\": \"Advantages/Benefits\",\n",
      "      \"thought_classification_explanation\": \"This section discusses another advantage of the Movement SDK: leveraging Move's inherent security.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it highlights a significant advantage of the Movement SDK in terms of security.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"*Accessing Move's Performance Capabilities:* The SDK enables access to the performance benefits of Move.\",\n",
      "      \"thought_classification\": \"Advantages/Benefits\",\n",
      "      \"thought_classification_explanation\": \"This section discusses another advantage of the Movement SDK: accessing Move's performance capabilities.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it highlights a significant advantage of the Movement SDK in terms of performance.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"In essence, the Movement SDK is a **foundational element** that allows Movement Labs to deliver its vision of a unified, secure, and high-performance blockchain ecosystem that leverages the strengths of both Move and Ethereum.\",\n",
      "      \"thought_classification\": \"Conclusion/Summary\",\n",
      "      \"thought_classification_explanation\": \"This sentence summarizes the main point of the post.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is positive as it reinforces the overall benefits and importance of the Movement SDK.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kk2ghs (r/CryptoMoonShots)\n",
      "Title: What are the best low-cap RWA tokens with actual utility  not just tokenized bonds?\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency - Real World Asset (RWA)\",\n",
      "  \"topic_explanation\": \"The post discusses the topic of Real World Assets (RWAs) in the context of cryptocurrencies.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The sentiment score is positive because the author expresses a favorable view towards the project REM and the concept of low-cap RWA tokens that are focused on utility rather than financial speculation.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"The Real World Asset (RWA) narrative is gaining serious momentum\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that the RWA narrative is gaining momentum.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the momentum of the RWA narrative is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"most of the capital and attention still seem locked into large institutional plays\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that most capital and attention are still focused on large institutional plays.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the focus of capital and attention is negative because it is locked into large institutional plays.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"While those use cases are valid\",\n",
      "      \"thought_classification\": \"Evaluation\",\n",
      "      \"thought_classification_explanation\": \"The author evaluates the validity of the use cases for large institutional plays.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the validity of the use cases is neutral.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019ve been exploring smaller projects and noticed a clear gap\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that they have explored smaller projects and noticed a clear gap.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the exploration of smaller projects is neutral.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"very few are focused on making RWAs accessible and useful for everyday people\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that very few projects are focused on making RWAs accessible and useful for everyday people.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the focus of projects on making RWAs accessible is negative because very few are doing so.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"especially in emerging markets where the tech could solve real problems\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that the technology could solve real problems in emerging markets.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the potential of the technology to solve problems is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"One project that stood out to me is REM\",\n",
      "      \"thought_classification\": \"Evaluation\",\n",
      "      \"thought_classification_explanation\": \"The author evaluates the project REM as standing out.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the evaluation of the project REM is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"It\\u2019s not just another wrapper for financial instruments\",\n",
      "      \"thought_classification\": \"Evaluation\",\n",
      "      \"thought_classification_explanation\": \"The author evaluates that REM is not just a wrapper for financial instruments.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the evaluation of REM being more than just a wrapper for financial instruments is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"REM focuses on tokenizing real-world services and infrastructure\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that REM focuses on tokenizing real-world services and infrastructure.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the focus of REM is neutral.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"like logistics, utilities, and land-based operations\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes the specific areas of focus for REM's tokenization.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the specific areas of focus is neutral.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The team is doxed\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that the team behind REM is doxed.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the team being doxed is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"active\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that the team behind REM is active.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the activity of the team is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"and appears to be working with real-world partners\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that REM appears to be working with real-world partners.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the partnerships of REM is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"to bridge blockchain with practical, on-the-ground use\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes that REM is working to bridge blockchain with practical, on-the-ground use.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the bridging of blockchain and practical use is positive.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"What I find compelling is how REM seems more focused on actual delivery than building hype\",\n",
      "      \"thought_classification\": \"Evaluation\",\n",
      "      \"thought_classification_explanation\": \"The author evaluates that the focus of REM seems more on actual delivery rather than building hype.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the evaluation of REM's focus is very positive because it is focused on actual delivery instead of building hype.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Curious what other low-cap RWA tokens you're watching that are building for real utility, not just financial speculation\",\n",
      "      \"thought_classification\": \"Question\",\n",
      "      \"thought_classification_explanation\": \"The author asks about other low-cap RWA tokens that are focused on building real utility rather than financial speculation.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment towards the question is neutral because it is a genuine inquiry.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1ewmkh5 (r/SatoshiStreetBets)\n",
      "Title: UPDATE: I Told you about $BUB one month ago at 890mk market cap, it's now at $30 Million\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a cryptocurrency project named Lil Bub on the Solana blockchain.\",\n",
      "  \"sentiment_score\": 5,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is very positive as it mentions significant returns for those who invested in the mentioned crypto and expresses optimism about its future potential, with a goal to reach a billion+ market cap.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"But it's not too late to buy\",\n",
      "      \"thought_classification\": \"Call-to-action\",\n",
      "      \"thought_classification_explanation\": \"The author is encouraging readers to invest in the mentioned cryptocurrency.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The call-to-action is presented with a positive tone, indicating that it's not too late to invest.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We are taking this to a billion+ market cap during this bull run\",\n",
      "      \"thought_classification\": \"Goal or Ambition\",\n",
      "      \"thought_classification_explanation\": \"The author expresses their ambition to reach a billion+ market cap for the mentioned cryptocurrency.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The goal is presented with a very positive tone, expressing optimism about the project's future potential.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I'll post again in another month, and you can see where we're at\",\n",
      "      \"thought_classification\": \"Future Expectation\",\n",
      "      \"thought_classification_explanation\": \"The author expresses their expectation to provide an update on the project's progress in a month.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The future expectation is presented with a neutral tone but has a positive connotation as it implies potential progress in the project.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kkf0l9 (r/CryptoCurrency)\n",
      "Title: Daily Crypto Discussion - May 12, 2025 (GMT+0)\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency Discussion\",\n",
      "  \"topic_explanation\": \"The post is about a daily discussion thread for cryptocurrencies on Reddit.\",\n",
      "  \"sentiment_score\": 3,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is neutral as it provides information and guidelines without expressing any strong positive or negative emotions.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Consider all information posted here with several liberal heaps of salt, and always cross check any information you may read on this thread with known sources.\",\n",
      "      \"thought_classification\": \"Warning\",\n",
      "      \"thought_classification_explanation\": \"The quoted chunk warns readers to be cautious about the information they encounter in the discussion thread.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the warning is negative as it advises caution and potential misinformation.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kj9l8p (r/CryptoMoonShots)\n",
      "Title: TrancheVest: The DeFi Risk Revolution That Lets YOU Control Your Crypto Destiny | Pick Your Risk Level, Let AI Do The Rest\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post discusses a new cryptocurrency project called TrancheVest, which is focused on decentralized finance (DeFi).\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is positive as it expresses excitement and anticipation for the potential success of the TrancheVest project.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"What's up community ! I've discovered a genuinely innovative project that's about to disrupt DeFi in a major way.\",\n",
      "      \"thought_classification\": \"Introduction\",\n",
      "      \"thought_classification_explanation\": \"The author introduces the topic of their post, which is a new cryptocurrency project called TrancheVest.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The introduction expresses excitement and anticipation for the potential success of the TrancheVest project.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"TrancheVest is building a revolutionary investment platform on Base Chain that lets you pick YOUR OWN risk level (from 1-100), then uses advanced AI agents to manage your crypto across different strategies.\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the investment platform that TrancheVest is building and how it allows users to choose their own level of risk.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the innovative and user-friendly aspects of TrancheVest's investment platform.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **Choose your risk score (1-100)** and the platform automatically allocates your funds\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author highlights one of the key features of TrancheVest's investment platform, which is the ability to choose a risk score and have the platform automatically allocate funds accordingly.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a convenient and user-friendly aspect of the platform.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **Specialized AI agents** manage each investment tranche 24/7\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author highlights another feature of TrancheVest's investment platform, which is the use of specialized AI agents to manage investments.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a convenient and efficient aspect of the platform.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **No more emotional trading** - let algorithms handle the stress\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author highlights a feature of TrancheVest's investment platform that eliminates the need for emotional trading by allowing algorithms to handle investments.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a stress-relieving aspect of the platform.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **Institutional-grade strategies** now available to average investors\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author highlights a feature of TrancheVest's investment platform that makes institutional-grade strategies accessible to average investors.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a democratizing aspect of the platform.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"This is where it gets interesting! The vault splits investments across four strategies:\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author introduces the flagship product of TrancheVest, which is the GreenTrancheVault, and describes how it invests funds across four different strategies.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the diversification and flexibility of the GreenTrancheVault.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **GreenStable**: For the conservative crowd (4-8% steady returns)\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author describes one of the strategies offered by the GreenTrancheVault, which is a low-risk strategy for conservative investors.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a safe and reliable option for conservative investors.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **GreenRWAs**: Real-world assets with medium risk (8-12%)\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author describes another strategy offered by the GreenTrancheVault, which is a moderate-risk strategy that invests in real-world assets.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a potentially profitable and diversified option for investors.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **GreenArbitrage**: Cross-exchange opportunities (12-18%)\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author describes another strategy offered by the GreenTrancheVault, which is a moderate-to-high-risk strategy that invests in cross-exchange opportunities.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a potentially profitable and high-risk option for investors.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **GreenSnipe**: High-risk/high-reward plays like MEV capture (15-25%+) \",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author describes the highest-risk strategy offered by the GreenTrancheVault, which invests in high-risk/high-reward plays.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a potentially very profitable but extremely high-risk option for investors.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The lower your risk score, the more you're allocated to the high-risk strategies. Higher score = more conservative allocation.\",\n",
      "      \"thought_classification\": \"Explanation\",\n",
      "      \"thought_classification_explanation\": \"The author explains how the risk score chosen by an investor affects their investment allocation within the GreenTrancheVault.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The explanation emphasizes the flexibility and personalization of the investment allocation process.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"They're allocating 5% of ALL tokens to community-driven marketing. Post quality content about TrancheVest and earn $TRANCHE tokens based on engagement! This is genius-level tokenomics.\",\n",
      "      \"thought_classification\": \"Feature\",\n",
      "      \"thought_classification_explanation\": \"The author highlights a feature of the TrancheVest project, which is a community rewards program for creating content about the project.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The feature is presented as a creative and incentivizing aspect of the project.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Use Cases That Actually Make Sense\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author lists various use cases for TrancheVest, emphasizing its broad applicability.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the versatility and accessibility of TrancheVest.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **Newbies**: Finally enter crypto without getting rekt\",\n",
      "      \"thought_classification\": \"Use Case\",\n",
      "      \"thought_classification_explanation\": \"The author lists a use case for TrancheVest, which is helping new investors avoid common pitfalls and losses in the cryptocurrency market.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The use case is presented as a beneficial and protective aspect of TrancheVest.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **DAOs**: Decentralized autonomous organizations can invest in a diversified portfolio with minimal effort\",\n",
      "      \"thought_classification\": \"Use Case\",\n",
      "      \"thought_classification_explanation\": \"The author lists another use case for TrancheVest, which is providing a convenient investment solution for decentralized autonomous organizations.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The use case is presented as a practical and efficient solution for decentralized organizations.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **Traditional Investors**: Access to institutional-grade strategies without the high minimums\",\n",
      "      \"thought_classification\": \"Use Case\",\n",
      "      \"thought_classification_explanation\": \"The author lists another use case for TrancheVest, which is providing traditional investors with access to previously exclusive investment opportunities.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The use case is presented as a democratizing aspect of TrancheVest.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"* **Institutional Investors**: A low-cost, scalable solution for managing and diversifying large portfolios\",\n",
      "      \"thought_classification\": \"Use Case\",\n",
      "      \"thought_classification_explanation\": \"The author lists another use case for TrancheVest, which is providing institutional investors with a cost-effective and scalable investment management solution.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The use case is presented as a beneficial and efficient solution for institutional investors.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Mind-blowing Tech: Risk management, portfolio optimization, and automated rebalancing\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the advanced technology behind TrancheVest, emphasizing its capabilities in risk management, portfolio optimization, and automatic rebalancing.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the sophistication and effectiveness of TrancheVest's technology.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Roadmap: Q3 2021 - Mainnet Launch, Q4 2021 - DEX Integrations, Q1 2022 - Mobile App, Q2 2022 - API and Institutional Solutions\",\n",
      "      \"thought_classification\": \"Plan\",\n",
      "      \"thought_classification_explanation\": \"The author outlines the roadmap for TrancheVest's development, including mainnet launch, DEX integrations, a mobile app, an API, and institutional solutions.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The plan is presented as a clear and ambitious vision for the future of TrancheVest.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Tokenomics: 5% of all tokens are allocated to community rewards, 10% to liquidity providers, and 85% to strategic partnerships and development.\",\n",
      "      \"thought_classification\": \"Plan\",\n",
      "      \"thought_classification_explanation\": \"The author provides details about the tokenomics of TrancheVest, including the allocation of tokens for community rewards, liquidity providers, strategic partnerships, and development.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The plan is presented as a fair and balanced distribution of tokens.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Team: A diverse group of experienced professionals with a passion for blockchain and finance\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the team behind TrancheVest, emphasizing their diversity, experience, and shared interests in blockchain and finance.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the competence and dedication of TrancheVest's team.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Advisors: Renowned experts in blockchain, finance, and marketing\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the advisors for TrancheVest, emphasizing their expertise in blockchain, finance, and marketing.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the knowledge and guidance provided by TrancheVest's advisors.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Partnerships: Collaborations with leading DeFi projects, exchanges, and blockchain platforms\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the partnerships forged by TrancheVest, emphasizing their strategic importance and the prestige of the collaborating entities.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the connections and opportunities provided by TrancheVest's partnerships.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Community: A vibrant, supportive, and growing group of enthusiasts\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the community surrounding TrancheVest, emphasizing its energy, camaraderie, and growth.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the positive and engaged nature of TrancheVest's community.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Social Media: Active presence on Twitter, Telegram, Discord, Reddit, and LinkedIn\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the social media presence of TrancheVest, emphasizing its activity and breadth across multiple platforms.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes TrancheVest's visibility and engagement on social media.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Website: Comprehensive information, resources, and updates about the project\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the website for TrancheVest, emphasizing its informative nature, resources, and updates.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the value and utility of TrancheVest's website.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Whitepaper: A detailed exploration of the project's technology, roadmap, and team\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes the whitepaper for TrancheVest, emphasizing its thoroughness, detail, and transparency.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The description emphasizes the comprehensiveness and trustworthiness of TrancheVest's whitepaper.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"TrancheVest is a decentralized investment platform that aims to democratize access to institutional-grade strategies for all investors.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author presents the core mission and purpose of TrancheVest, emphasizing its focus on democratizing investment opportunities.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The statement emphasizes the inclusive and empowering nature of TrancheVest's mission.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1fbbhlv (r/SatoshiStreetBets)\n",
      "Title: We're still flying under the radar\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a cryptocurrency named Kendu Inu.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The post has a positive sentiment as the author expresses trust in the project, its developer, and the community. The author also highlights unique features of the project such as organic marketing and upcoming debit cards and NFT marketplace.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu Inu has been making plays in the shadow recently.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that Kendu Inu has been active recently.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Let me introduce you to the face of Kendu, Miazaki.\",\n",
      "      \"thought_classification\": \"Introduction\",\n",
      "      \"thought_classification_explanation\": \"The author is introducing the reader to Miazaki who is associated with Kendu.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The introduction does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Miazaki is our developer who is highly regarded by some and criticised by others.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that Miazaki, the developer of Kendu, is both respected and criticized.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Critics do not like that he uses unhinged and vulgar language, as well as keeping secrets.\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for the criticism of Miazaki.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement expresses a negative sentiment as it mentions unhinged and vulgar language and keeping secrets.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Personally, I think his mannerisms are funny\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating their personal opinion about Miazaki's mannerisms.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The statement expresses a positive sentiment as the author finds Miazaki's mannerisms funny.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"and while some do not it is not a reason to disown the project.\",\n",
      "      \"thought_classification\": \"Justification\",\n",
      "      \"thought_classification_explanation\": \"The author is justifying their support for Kendu despite criticism of Miazaki.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"A major reason why I trust Kendu and the developer\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for their trust in Kendu and its developer.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"is because we don't follow a typical memecoin roadmap.\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for their trust in Kendu and its developer.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We haven't paid any KOL's and all marketing is done organically\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for their trust in Kendu and its developer.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"as you've probably seen community members shilling on X (twitter), Reddit, TikTok and Stocktwits.\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for their trust in Kendu and its developer.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Furthermore, we recently announced Kendu debit cards\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they have recently announced Kendu debit cards.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \", and have a NFT Marketplace which will launch soon.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they also have an upcoming NFT marketplace.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"These recently announced debit cards and NFT marketplace also reinforce the different path we are taking.\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for their trust in Kendu and its developer.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We aren't waiting for anyone to come and save us.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they are not waiting for external help.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We aren't sitting around waiting for someone like Elon, Vitalik or a big influencer to randomly tweet about us.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they are not waiting for influential figures to promote Kendu.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We ARE actively progressing the project to a point where Kendu when it was first created is COMPLETELY different to what we are now regardless of what the chart says.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they are actively progressing Kendu and it has significantly changed since its inception.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"For me, this places a lot of trust in our developer and the community (which is the biggest alpha of kendu) and is why I've been holding for months now without selling a single coin.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they have trust in the developer and the community, and have held Kendu for months without selling.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Anyway, you may ask 'how we have flown under the radar?' If you look on crypto twitter, you'll likely see no one talking about us.\",\n",
      "      \"thought_classification\": \"Question and Answer\",\n",
      "      \"thought_classification_explanation\": \"The author is asking a question and providing an answer.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"All these big twitter influencers all are being paid by developers to shill a coin.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that big Twitter influencers are being paid to promote coins.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement expresses a negative sentiment as it mentions being paid to shill coins.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The reason they don't shill us is they do not currently hold a bag of Kendu.\",\n",
      "      \"thought_classification\": \"Reason\",\n",
      "      \"thought_classification_explanation\": \"The author is providing a reason for why big Twitter influencers do not promote Kendu.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"This means that we have gained our marketcap WITHOUT outside help.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they have gained their market capital without external help.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"When we inevitably start to be recognised by influencers think of the marketcap we can grow to.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that when they are recognized by influencers, their market capital will grow.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"That's all I had to say, thanks for reading.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they have finished their post.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Links to main twitter and dev's twitter:\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating that they have provided links to their Twitter accounts.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"main twitter: https://twitter.com/kenduofficial\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating the link to their main Twitter account.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"dev's twitter: https://twitter.com/kendudev\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is stating the link to their developer's Twitter account.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The statement does not express any sentiment.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kffeln (r/CryptoMoonShots)\n",
      "Title: My First Web3 Trading Disaster: A Lesson to Learn From - 2025-05-05\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency Trading Strategy\",\n",
      "  \"topic_explanation\": \"The post discusses a personal experience in Web3 trading, shares insights on a potential strategy for investing in cryptocurrencies, and mentions tools like TradingView and AIQuant.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The post has a generally positive sentiment as it provides helpful advice, acknowledges mistakes, and emphasizes the importance of patience and strategy in cryptocurrency trading.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"So first things first, if you're looking to make a quick buck, you're gonna have a bad time.\",\n",
      "      \"thought_classification\": \"Caution\",\n",
      "      \"thought_classification_explanation\": \"The author warns against expecting quick profits in the cryptocurrency market.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought expresses a neutral sentiment as it is stating a fact about the nature of trading in the cryptocurrency market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Let's talk about one of my biggest mess-ups.\",\n",
      "      \"thought_classification\": \"Personal Experience\",\n",
      "      \"thought_classification_explanation\": \"The author shares a personal experience in the cryptocurrency market.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought has a negative sentiment as it discusses a mistake made by the author.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Patience, my friends.\",\n",
      "      \"thought_classification\": \"Advice\",\n",
      "      \"thought_classification_explanation\": \"The author emphasizes the importance of patience in the cryptocurrency market.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought has a positive sentiment as it offers advice for success in the cryptocurrency market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Here's my two cents on a decent strategy:\",\n",
      "      \"thought_classification\": \"Strategy\",\n",
      "      \"thought_classification_explanation\": \"The author shares their investment strategy for the cryptocurrency market.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought has a positive sentiment as it offers helpful advice on investing in the cryptocurrency market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Stay safe out there and happy trading!\",\n",
      "      \"thought_classification\": \"Encouragement\",\n",
      "      \"thought_classification_explanation\": \"The author encourages the reader to stay safe and have a good trading experience.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought has a very positive sentiment as it offers encouragement to the reader.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1j36z46 (r/altcoin)\n",
      "Title: The time to load up on conviction plays is now - Kendu is ready\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a specific cryptocurrency called Kendu Inu and its potential for growth despite the current market downturn.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"While the post expresses fear and uncertainty about the current state of the crypto market, it ultimately encourages readers to invest in Kendu Inu as a 'conviction play' and presents it as a strong investment opportunity. This suggests a positive sentiment towards Kendu Inu.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Many of us have been feeling the fear these past few weeks by holding during this market downturn.\",\n",
      "      \"thought_classification\": \"Fear\",\n",
      "      \"thought_classification_explanation\": \"The author expresses fear about the current state of the crypto market and the potential losses they may incur.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is negative as it expresses fear about the market downturn.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Market manipulation from exchanges has happened time and time again.\",\n",
      "      \"thought_classification\": \"Market Manipulation\",\n",
      "      \"thought_classification_explanation\": \"The author suggests that the current market downturn is due to market manipulation by exchanges.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is negative as it suggests market manipulation.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Well, the same thing has happened over and over again.\",\n",
      "      \"thought_classification\": \"Historical Comparison\",\n",
      "      \"thought_classification_explanation\": \"The author compares the current market downturn to previous downturns in the crypto market.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is neutral as it simply makes a comparison without expressing a strong opinion.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Simply put, this is market manipulation, and it is not unique.\",\n",
      "      \"thought_classification\": \"Market Manipulation\",\n",
      "      \"thought_classification_explanation\": \"The author reiterates their belief that the current market downturn is due to market manipulation.\",\n",
      "      \"thought_sentiment\": 1,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is negative as it suggests market manipulation.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"To the point where you are thinking that it is over, bull run has ended, there will be no alt season.\",\n",
      "      \"thought_classification\": \"Despair\",\n",
      "      \"thought_classification_explanation\": \"The author suggests that some people may feel despair due to the current market downturn and believe that the bull run is over and there will be no alt season.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is negative as it expresses despair about the market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Well that exactly rings true during this fearful market.\",\n",
      "      \"thought_classification\": \"Market Fear\",\n",
      "      \"thought_classification_explanation\": \"The author acknowledges the current market fear.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is negative as it acknowledges the current market fear.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"As Warren Buffet puts it 'Be fearful when others are greedy and be greedy when others are fearful'\",\n",
      "      \"thought_classification\": \"Investment Advice\",\n",
      "      \"thought_classification_explanation\": \"The author provides investment advice based on a quote from Warren Buffet.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it provides investment advice that could lead to potential profits.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We are so close to alt season now and with everything on fire sales it is the time to load up on strong conviction plays.\",\n",
      "      \"thought_classification\": \"Alt Season\",\n",
      "      \"thought_classification_explanation\": \"The author suggests that we are close to an alt season and encourages readers to invest in 'strong conviction plays'.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it encourages investment in 'strong conviction plays' and suggests an upcoming alt season.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The community has grown for over a year now, building with it an establishment and foundation that is essentially unseen elsewhere.\",\n",
      "      \"thought_classification\": \"Community Growth\",\n",
      "      \"thought_classification_explanation\": \"The author discusses the growth of Kendu Inu's community over the past year.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it highlights the growth and strength of Kendu Inu's community.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Next is the massive IRL movement that is brewing.\",\n",
      "      \"thought_classification\": \"IRL Movement\",\n",
      "      \"thought_classification_explanation\": \"The author discusses an 'IRL' (in real life) movement related to Kendu Inu.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it suggests a strong 'IRL' movement for Kendu Inu.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"This community has demonstrated its sincere tenacity in order for success during this market downturn and its own consolidation period.\",\n",
      "      \"thought_classification\": \"Community Tenacity\",\n",
      "      \"thought_classification_explanation\": \"The author discusses the tenacity of Kendu Inu's community during the current market downturn.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it highlights the tenacity of Kendu Inu's community during difficult times.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"It is simply a matter of time and if you were ever looking for a good entry, now is quite likely the best time ever in the lifetime of Kendu.\",\n",
      "      \"thought_classification\": \"Investment Opportunity\",\n",
      "      \"thought_classification_explanation\": \"The author suggests that now is a good time to invest in Kendu Inu.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it suggests an investment opportunity in Kendu Inu.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Join us, join the movement. We await you.\",\n",
      "      \"thought_classification\": \"Invitation\",\n",
      "      \"thought_classification_explanation\": \"The author invites readers to join Kendu Inu's community.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it invites readers to join Kendu Inu's community.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kkmpdz (r/CryptoMarkets)\n",
      "Title: I came across a new coin - Zump \n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a specific cryptocurrency called Zump. It discusses the coin, its origins, and who is behind it.\",\n",
      "  \"sentiment_score\": 2,\n",
      "  \"sentiment_explanation\": \"The post expresses skepticism and uncertainty about the legitimacy of the Zump coin, indicating a negative sentiment.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019m not sure if it\\u2019s some kind of joke or a legitimate project.\",\n",
      "      \"thought_classification\": \"Doubt\",\n",
      "      \"thought_classification_explanation\": \"The user is unsure about the authenticity of Zump coin.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The doubt expressed is negative in nature.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"it seems to be meme and satire-driven\",\n",
      "      \"thought_classification\": \"Analysis\",\n",
      "      \"thought_classification_explanation\": \"The user is analyzing the nature of Zump coin based on its characteristics.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The analysis indicates a negative sentiment towards the coin.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Are there any interesting features, or is it just another meme coin?\",\n",
      "      \"thought_classification\": \"Question\",\n",
      "      \"thought_classification_explanation\": \"The user is asking for information about the unique features of Zump coin.\",\n",
      "      \"thought_sentiment\": 0,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The question is neutral in nature.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kispqs (r/CryptoMoonShots)\n",
      "Title: $KENDU- The opportunity of a lifetime!\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency - Promotion\",\n",
      "  \"topic_explanation\": \"The post is about a specific cryptocurrency called Kendu Inu, and it is promoting the coin by highlighting its unique features such as community involvement, products associated with the coin, and recent positive developments.\",\n",
      "  \"sentiment_score\": 5,\n",
      "  \"sentiment_explanation\": \"The post is highly positive in tone. The author expresses excitement about Kendu Inu being a 'real gem' and 'opportunity of a lifetime', and encourages readers to explore the coin and its community.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"$Kendu isn\\u2019t just another memecoin.\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author is making a statement that Kendu Inu is not like other meme coins.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive, as the author implies that Kendu Inu stands out from other meme coins.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu is a brand, a community and a way for people to create and express themselves how they see fit.\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"The author describes Kendu Inu as more than just a coin; it's also a brand and a community that allows individuals to express themselves.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive, as the author emphasizes the unique aspects of Kendu Inu and its community.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"On top of that Kendu is backed by a community which is working tirelessly to make sure it succeeds!\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author makes a statement about the community's dedication to ensuring the success of Kendu Inu.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive, as the author highlights the community's hard work and dedication to Kendu Inu.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We don\\u2019t gamble, we work with a \\u201cwe kendu it\\u201d attitude!\",\n",
      "      \"thought_classification\": \"Statement\",\n",
      "      \"thought_classification_explanation\": \"The author makes a statement about the community's approach to working with Kendu Inu.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive, as the author emphasizes the community's commitment and determination towards Kendu Inu.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kjhbn3 (r/CryptoMarkets)\n",
      "Title: Is Shardeum quietly cracking the scaling puzzle that Ethereums still wrestling with?\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post discusses the cryptocurrency Shardeum, specifically mentioning its dynamic state sharding technology and its token $SHM.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is generally positive as the author expresses interest in the technology behind Shardeum and considers it a potential game-changer.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Picture a blockchain that automatically splits and reshuffles itself as more users or nodes jump in, sorta like how cloud servers scale on the fly.\",\n",
      "      \"thought_classification\": \"Innovation\",\n",
      "      \"thought_classification_explanation\": \"The author describes Shardeum's dynamic state sharding technology as innovative because it allows the blockchain to automatically adjust itself based on network usage, similar to how cloud servers scale.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is very positive as the author finds the idea of dynamic state sharding intriguing and potentially revolutionary.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Did anyone here messed around with Shardeum\\u2019s testnet or took a closer look at how this sharding stuff works?\",\n",
      "      \"thought_classification\": \"Inquiry\",\n",
      "      \"thought_classification_explanation\": \"The author asks for information and experiences related to Shardeum's testnet and the sharding technology it employs.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is neutral as the author simply asks a question without expressing any particular sentiment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019m wondering if this is a legit game-changer or just another cool idea that sounds better in theory than in practice.\",\n",
      "      \"thought_classification\": \"Doubt\",\n",
      "      \"thought_classification_explanation\": \"The author expresses doubt about whether Shardeum's technology will be effective in practice, despite its promising potential.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The thought is negative as the author questions the practicality of Shardeum's technology.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1jo4yse (r/altcoin)\n",
      "Title: #digi\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a specific cryptocurrency named Digicoin (DIGI), and it includes the current price ($0.0\\u20878) and percentage change (-14.04%)\",\n",
      "  \"sentiment_score\": 2,\n",
      "  \"sentiment_explanation\": \"The post shows a negative sentiment as the cryptocurrency is experiencing a decrease in value (price drop of -14.04%)\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Hey, check this out!\",\n",
      "      \"thought_classification\": \"Attention-grabber\",\n",
      "      \"thought_classification_explanation\": \"The user is trying to grab the attention of their audience by using an exclamation mark and 'check this out!' phrase.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the attention-grabber is neutral as it's just trying to get the reader's attention.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1khltr2 (r/CryptoMarkets)\n",
      "Title: Am I the only one who finds this recovery surprising?\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency Market Analysis\",\n",
      "  \"topic_explanation\": \"The post discusses the current state and potential future developments in the cryptocurrency market, specifically focusing on Bitcoin (BTC) prices.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"While the author acknowledges some uncertainty in the market and expresses caution about the tariff situation, overall the tone of the post is positive due to the recovery of S&P and BTC prices.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Despite all the uncertainty in the market, S&P recovers to early April prices. Btc recovers to 99k.\",\n",
      "      \"thought_classification\": \"Market Recovery\",\n",
      "      \"thought_classification_explanation\": \"The author mentions the recovery of both S&P and Bitcoin prices, indicating a positive market trend.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The recovery of both S&P and BTC prices is generally considered a positive development in the market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"90 day pause in tariffs is good news but not for long - yet the market seems very bullish.\",\n",
      "      \"thought_classification\": \"Tariff Pause and Market Bullishness\",\n",
      "      \"thought_classification_explanation\": \"The author acknowledges a temporary pause in tariffs as positive news, but also expresses concern about its long-term impact on the market.\",\n",
      "      \"thought_sentiment\": 2,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The author expresses caution regarding the tariff situation and the long-term implications for the market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I'm not complaining of course but it's honestly surprising\",\n",
      "      \"thought_classification\": \"Surprise and Acknowledgement\",\n",
      "      \"thought_classification_explanation\": \"The author acknowledges the positive market developments, but also expresses surprise at their occurrence.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The author is neither expressing strong positivity nor negativity, but rather acknowledging the surprising nature of the market recovery.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1f3eo0m (r/SatoshiStreetBets)\n",
      "Title: Kendu Inu now has an officially verified account on StockTwits\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a cryptocurrency named Kendu Inu.\",\n",
      "  \"sentiment_score\": 5,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is very positive due to the excitement and anticipation expressed towards the potential growth and success of the Kendu Inu project.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"More big news for the Kendu Inu ecosystem as we recently obtained a verified account on StockTwits\",\n",
      "      \"thought_classification\": \"News\",\n",
      "      \"thought_classification_explanation\": \"The post mentions new development or event related to Kendu Inu, which can be classified as news.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the news is very positive due to the excitement expressed about obtaining a verified account on StockTwits.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I advise you to not ignore the obvious signs when it comes to Kendu's potential.\",\n",
      "      \"thought_classification\": \"Advice\",\n",
      "      \"thought_classification_explanation\": \"The post provides advice or recommendation about investing in Kendu Inu.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the advice is very positive due to the optimism expressed about Kendu's potential.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Not only did we get a verified account, but the Co-Founder and CEO of Stocktwits Howard Lindzon (who has over 200k followers on StockTwits) shared a post specifically mentioning Kendu.\",\n",
      "      \"thought_classification\": \"Endorsement\",\n",
      "      \"thought_classification_explanation\": \"The post mentions an endorsement or support from a well-known figure in the stock and crypto trading industry, which can be classified as endorsement.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the endorsement is very positive due to the excitement expressed about Kendu being mentioned by a prominent figure in the industry.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"It's just so obvious where Kendu Inu is headed\",\n",
      "      \"thought_classification\": \"Prediction\",\n",
      "      \"thought_classification_explanation\": \"The post makes a prediction or forecast about the future direction of Kendu Inu.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the prediction is very positive due to the optimism expressed about Kendu's future direction.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Thanks to market conditions, it's the perfect entry point yet again to get into Kendu as well!\",\n",
      "      \"thought_classification\": \"Opportunity\",\n",
      "      \"thought_classification_explanation\": \"The post mentions an opportunity or advantage for investors to invest in Kendu Inu.\",\n",
      "      \"thought_sentiment\": 5,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of the opportunity is very positive due to the excitement expressed about the current market conditions being favorable for investing in Kendu.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1jbbl6o (r/altcoin)\n",
      "Title: Why Kendu is my number 1 pick and it should be yours to\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is about a cryptocurrency named Kendu, discussing its current market performance, recent developments, and future potential.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The post is overall positive about the cryptocurrency Kendu. The author expresses optimism about the coin's daily holder growth, distribution strategy, and potential for success during a future bull run. Additionally, the author highlights various initiatives such as merchandise, events, and collaborations that are intended to boost the visibility of Kendu.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Although it is a very tough and desolate market atm, Kendu is still gaining holders daily which is a great sign.\",\n",
      "      \"thought_classification\": \"Positive\",\n",
      "      \"thought_classification_explanation\": \"The author acknowledges the tough market conditions but finds positivity in Kendu's ability to gain new holders despite these circumstances.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The author finds positivity in the fact that Kendu is gaining holders daily despite a tough market.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Distribution will be a very strong driving factor for success when the bull run comes, as it means the sell pressure will be very low for weeks.\",\n",
      "      \"thought_classification\": \"Positive\",\n",
      "      \"thought_classification_explanation\": \"The author expresses optimism about Kendu's distribution strategy and its potential impact on success during a bull run.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The author finds positivity in the idea that Kendu's distribution strategy will lead to low sell pressure during a bull run.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"This gives people the chance to wear the Kendu mask when creating other non crypto related social media content.\",\n",
      "      \"thought_classification\": \"Positive\",\n",
      "      \"thought_classification_explanation\": \"The author sees potential in Kendu's branding efforts, such as allowing users to create and wear masks for non-crypto content.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The author finds positivity in the idea that Kendu's branding efforts could help the coin go viral and capture various corners of the internet.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu is a super strong cult memecoin ran entirely by the community.\",\n",
      "      \"thought_classification\": \"Positive\",\n",
      "      \"thought_classification_explanation\": \"The author expresses confidence in Kendu as a strong memecoin, emphasizing its community-driven nature.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The author finds positivity in Kendu's reputation as a strong memecoin and its community-driven nature.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"In the middle of Feb 2021 Shiba was hovering around 14 million mc. Kendu is very close to this point now.\",\n",
      "      \"thought_classification\": \"Neutral\",\n",
      "      \"thought_classification_explanation\": \"The author compares Kendu's market cap with Shiba from February 2021, but the comparison does not inherently convey a positive or negative sentiment.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The author does not express positivity or negativity in comparing Kendu's market cap with Shiba from February 2021.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kmdxdh (r/CryptoMarkets)\n",
      "Title: What is this token about\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post is discussing a specific Solana-based liquidity pool, which falls under the broader category of Cryptocurrency.\",\n",
      "  \"sentiment_score\": 3,\n",
      "  \"sentiment_explanation\": \"The sentiment is neutral as it does not express any clear positive or negative emotions. The user is asking for information and seems to be curious rather than expressing excitement or concern.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"I hope this message finds you well.\",\n",
      "      \"thought_classification\": \"Polite Greeting\",\n",
      "      \"thought_classification_explanation\": \"The user starts the post with a polite greeting, which is a common practice in written communication.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is neutral as it expresses politeness and friendliness.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019m reaching out to gather insights about a specific link related to a Solana-based liquidity pool that has come to my attention:\",\n",
      "      \"thought_classification\": \"Request for Information\",\n",
      "      \"thought_classification_explanation\": \"The user is asking for information about a specific Solana-based liquidity pool.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is neutral as the user is simply asking for information.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"https://geckoterminal.com/solana/pools/ANY6m193bZQppKPggKS8NN4nF9cq7YJHroYPdXws9G5k\",\n",
      "      \"thought_classification\": \"Link Sharing\",\n",
      "      \"thought_classification_explanation\": \"The user shares a link related to the Solana-based liquidity pool.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is neutral as it does not express any emotions.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019m new to this meme space I appreciate it if someone could give me a little context about this meme coin\",\n",
      "      \"thought_classification\": \"Request for Explanation\",\n",
      "      \"thought_classification_explanation\": \"The user mentions that they are new to the 'meme space' and asks for some context about the mentioned Solana-based liquidity pool.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is neutral as the user is simply asking for information.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1kg3x4j (r/CryptoMoonShots)\n",
      "Title: Introducing Prop\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency Presale\",\n",
      "  \"topic_explanation\": \"The post discusses the presale of a cryptocurrency token named PROP, which is designed to bridge decentralized finance and real-world trading opportunities. The post provides details about the token's utility, roadmap, and features.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The sentiment of this post is positive as it highlights the potential benefits and features of the PROP token, such as its clear tokenomics, instant token delivery, and purpose-built design to enhance the Financia Futures platform.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"The presale for PROP is just around the corner, starting today at 7:33pm UTC!\",\n",
      "      \"thought_classification\": \"Announcement\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text announces the start of the presale for the PROP token.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it announces the start of the presale for the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"PROP is a utility token designed to bridge the gap between decentralized finance and real-world trading opportunities.\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text provides a description of the purpose of the PROP token, which is to connect decentralized finance with real-world trading.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it describes the purpose and potential benefits of the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Built on the Solana blockchain, $PROP offers investors and traders a transparent, secure, and scalable entry point into the ecosystem of Financia Futures.\",\n",
      "      \"thought_classification\": \"Description\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text describes the technical details of the PROP token, such as its blockchain platform and the benefits it offers to investors and traders.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the technical benefits and potential uses of the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Unlike speculative tokens with unclear backing or vague promises, PROP is purpose-built to enhance and expand the Financia Futures platform.\",\n",
      "      \"thought_classification\": \"Comparison\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text compares the PROP token to other speculative tokens with unclear backing or vague promises, emphasizing its purpose-built design.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the unique and purposeful design of the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Backed by Financia Futures, a global proprietary trading firm that funds real traders and supports genuine financial growth.\",\n",
      "      \"thought_classification\": \"Background\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text provides background information about the company backing the PROP token, Financia Futures.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the backing and support provided by Financia Futures for the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"$PROP serves as a gateway for the community to connect with Financia Futures\\u2019 prop trading platform.\",\n",
      "      \"thought_classification\": \"Function\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text explains the function of the PROP token, which is to serve as a gateway for the community to access Financia Futures' prop trading platform.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the potential benefits and uses of the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Additional listings will follow organically, based on demand and ecosystem growth.\",\n",
      "      \"thought_classification\": \"Future Plans\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text discusses future plans for the PROP token, such as additional listings on other exchanges.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the potential for growth and expansion of the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"The roadmap includes upcoming features such as staking and community rewards.\",\n",
      "      \"thought_classification\": \"Future Plans\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text discusses future plans for the PROP token, such as upcoming features like staking and community rewards.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the potential benefits and features of the PROP token in the future.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"BUILT FOR TRUST\",\n",
      "      \"thought_classification\": \"Assurance\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text provides assurances about the trustworthiness and security of the PROP token.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it provides assurances about the trustworthiness and security of the PROP token.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"ROADMAP\",\n",
      "      \"thought_classification\": \"Roadmap\",\n",
      "      \"thought_classification_explanation\": \"This chunk of text introduces a roadmap for the development and growth of the PROP token.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the potential for growth and development of the PROP token.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1jgg19h (r/altcoin)\n",
      "Title: Liquid Staking & Restaking The Next Frontier for passive income\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency Investment\",\n",
      "  \"topic_explanation\": \"The post discusses the trend of liquid staking and restaking projects, specifically in relation to their potential investment value.\",\n",
      "  \"sentiment_score\": 3,\n",
      "  \"sentiment_explanation\": \"The sentiment is neutral as the author expresses curiosity, concern, and a desire for advice without expressing strong positive or negative feelings.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019ve been noticing a growing wave of projects popping up around liquid staking and restaking\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author has observed a trend in the crypto market.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is neutral as the author presents this observation without expressing strong positive or negative feelings.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"They seem to generate a ton of buzz and visibility\",\n",
      "      \"thought_classification\": \"Observation\",\n",
      "      \"thought_classification_explanation\": \"The author observes the level of attention these projects are receiving.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is neutral as the author presents this observation without expressing strong positive or negative feelings.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"That kind of hype can be a double-edged sword\",\n",
      "      \"thought_classification\": \"Analysis\",\n",
      "      \"thought_classification_explanation\": \"The author analyzes the potential risks associated with the high level of attention these projects are receiving.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": false,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is neutral as the author presents this analysis without expressing strong positive or negative feelings.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Do you think these could be a smart bet for investors\",\n",
      "      \"thought_classification\": \"Question\",\n",
      "      \"thought_classification_explanation\": \"The author asks for advice on whether to invest in these projects.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment is neutral as the author presents this question without expressing strong positive or negative feelings.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1jh6ur5 (r/altcoin)\n",
      "Title: Kendu Conquest - Yes Kendu Has a Boardgame\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post discusses a cryptocurrency named Kendu, its features, growth, and various events related to it.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is positive. The author expresses confidence in the growth potential of Kendu and encourages others to invest in it.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu is a memecoin that is on 3 chains: Ethereum, Solana and BASE, Kendu is focused on organic, non KOL, growth. We are entirely community led, focusing on both social media and IRL events/products.\",\n",
      "      \"thought_classification\": \"Background Information\",\n",
      "      \"thought_classification_explanation\": \"The author provides background information about the cryptocurrency Kendu, its chains, focus on organic growth, and community leadership.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it discusses the focus on organic growth and community leadership.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"We have around 18k holders and this is growing even in this market.\",\n",
      "      \"thought_classification\": \"Current State\",\n",
      "      \"thought_classification_explanation\": \"The author mentions the current number of Kendu holders, which is approximately 18k.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is neutral as it discusses the current number of Kendu holders.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu ATH Market Cap was 283 million, back in June 2024, which is around a 30x from this value.\",\n",
      "      \"thought_classification\": \"Past Performance\",\n",
      "      \"thought_classification_explanation\": \"The author mentions the All-Time High (ATH) Market Cap of Kendu in June 2024 and its growth since then.\",\n",
      "      \"thought_sentiment\": 3,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is neutral as it discusses the past performance of Kendu.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"However, when ETH wakes up, we are pretty confident Kendu is guaranteed to reach ATH.\",\n",
      "      \"thought_classification\": \"Future Potential\",\n",
      "      \"thought_classification_explanation\": \"The author expresses confidence in the future potential of Kendu, particularly when Ethereum (ETH) recovers.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it expresses confidence in the future potential of Kendu.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"In my opinion(not financial advice), buying Kendu at this price removes all risk from the investment.\",\n",
      "      \"thought_classification\": \"Advice\",\n",
      "      \"thought_classification_explanation\": \"The author provides their personal opinion (not financial advice) about investing in Kendu.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it suggests that buying Kendu at the current price removes all risk from the investment.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"In the community, we are planning to have multiple more chill VCs in the telegram, which I welcome everyone to join, whether you are a holder or not.\",\n",
      "      \"thought_classification\": \"Community Engagement\",\n",
      "      \"thought_classification_explanation\": \"The author discusses plans for increasing community engagement by adding more VCs (Venture Capitalists) to the telegram.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it encourages community engagement.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu has many IRL products: Kendu Energy, Kendu Coffee, Kendu Merch, Kendu animation, Kendu games, Kendu Gum, Kendu pendants and recently Kendu Beer.\",\n",
      "      \"thought_classification\": \"Products/Services\",\n",
      "      \"thought_classification_explanation\": \"The author lists various physical and digital products/services offered by Kendu.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the variety of products/services offered by Kendu.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu was at Miami Art Basel, NoSleep305 was present and even created a Kendu Mural in a very busy part of Miami\",\n",
      "      \"thought_classification\": \"Events/Presence\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu was present at Miami Art Basel and had a mural created by NoSleep305.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's presence at a significant event.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu is a head sponsor at Miami Music Week.\",\n",
      "      \"thought_classification\": \"Events/Presence\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu is the head sponsor of Miami Music Week.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's presence at a significant event.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu has multiple international communities incase English is not your first language:\\ud83c\\uddee\\ud83c\\uddf3\\u00a0India,\\ud83c\\uddea\\ud83c\\uddf8\\u00a0Spanish,\\ud83c\\udde6\\ud83c\\uddea\\u00a0Arabic, \\ud83c\\uddeb\\ud83c\\uddf7\\u00a0French, \\ud83c\\udde9\\ud83c\\uddea\\u00a0German, \\ud83c\\uddf5\\ud83c\\uddf9\\u00a0Portuguese, \\ud83c\\uddf7\\ud83c\\uddfa\\u00a0Russian, \\ud83c\\udde8\\ud83c\\uddf3\\u00a0Chinese, \\ud83c\\uddef\\ud83c\\uddf5\\u00a0Japanese, \\ud83c\\uddee\\ud83c\\udde9\\u00a0Indonesian, \\ud83c\\uddf0\\ud83c\\uddf7\\u00a0Korean, \\ud83c\\uddf9\\ud83c\\udded\\u00a0Thai, \\ud83c\\udde7\\ud83c\\uddf7\\u00a0Portuguese (Brazil), \\ud83c\\uddf2\\ud83c\\uddfd\\u00a0Spanish (Mexico)\",\n",
      "      \"thought_classification\": \"International Presence\",\n",
      "      \"thought_classification_explanation\": \"The author mentions the international communities for Kendu, catering to various languages.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's international presence.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu has partnered with various brands and influencers to increase its reach.\",\n",
      "      \"thought_classification\": \"Partnerships\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu has partnered with various brands and influencers.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's partnerships with brands and influencers.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu has also created a scholarship program for students in need.\",\n",
      "      \"thought_classification\": \"Social Responsibility\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu has a scholarship program for students in need.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's social responsibility efforts.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu has a strong focus on sustainability and eco-friendly practices.\",\n",
      "      \"thought_classification\": \"Environmental Impact\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu has a strong focus on sustainability and eco-friendly practices.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's focus on sustainability and eco-friendly practices.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu has a mobile app available for iOS and Android devices.\",\n",
      "      \"thought_classification\": \"Technology\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu has a mobile app available for both iOS and Android devices.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights the availability of Kendu's mobile app.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Kendu also offers a variety of educational resources to help users understand the cryptocurrency market.\",\n",
      "      \"thought_classification\": \"Education\",\n",
      "      \"thought_classification_explanation\": \"The author mentions that Kendu offers various educational resources to help users understand the cryptocurrency market.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The sentiment of this thought is positive as it highlights Kendu's educational resources.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Structured Output for Post ID: 1jwiu0g (r/altcoin)\n",
      "Title: Whats the Deal with $PROMPT Token? Diving into Wayfinder, the AI-Blockchain Powerhouse!\n",
      "{\n",
      "  \"topic_classification\": \"Cryptocurrency\",\n",
      "  \"topic_explanation\": \"The post discusses a specific cryptocurrency token ($PROMPT) and its associated project, Wayfinder. The content focuses on the features of the project and its native token.\",\n",
      "  \"sentiment_score\": 4,\n",
      "  \"sentiment_explanation\": \"The sentiment of the post is positive as it expresses excitement about the potential of $PROMPT token and the Wayfinder project, using terms like 'spicy', 'serious sauce', and 'buzz'. The author also mentions that they are keeping tabs on this project, indicating a level of interest or optimism.\",\n",
      "  \"thoughts\": [\n",
      "    {\n",
      "      \"quoted_chunk\": \"If you\\u2019re vibing with AI, blockchain, or just chasing the next big crypto wave\",\n",
      "      \"thought_classification\": \"Expresses interest in AI, blockchain, and cryptocurrency\",\n",
      "      \"thought_classification_explanation\": \"The author mentions a specific interest in three areas: AI, blockchain, and the potential for finding the next big crypto wave. This suggests that they are actively following developments in these fields.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it expresses a desire to be involved with the latest advancements in AI and blockchain technology.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"Wayfinder\\u2019s mission? Make Web3 as smooth as your favorite app\",\n",
      "      \"thought_classification\": \"Expresses expectation for user-friendly experience\",\n",
      "      \"thought_classification_explanation\": \"The author expects Wayfinder to provide a smooth and user-friendly experience, similar to their favorite apps.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it expresses a desire for a seamless and user-friendly experience with Wayfinder.\"\n",
      "    },\n",
      "    {\n",
      "      \"quoted_chunk\": \"I\\u2019m keeping tabs on this one\\u2014what about you? Got thoughts on Wayfinder or $PROMPT? Let\\u2019s hear \\u2018em!\",\n",
      "      \"thought_classification\": \"Expresses interest in engaging with the community\",\n",
      "      \"thought_classification_explanation\": \"The author indicates that they are following the development of Wayfinder and invites others to share their thoughts on the project.\",\n",
      "      \"thought_sentiment\": 4,\n",
      "      \"positivity\": true,\n",
      "      \"thought_sentiment_explanation\": \"The thought is positive as it expresses a desire to engage with the community and discuss the potential of Wayfinder and $PROMPT.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m posts_per_subreddit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m  \u001b[38;5;66;03m# Increased to get more posts\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load or create data, appending new posts\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_train, y_train, X_test, y_test, df_train, df_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_create_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubreddits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposts_per_subreddit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Reshape y_train and y_test to 1D if necessary\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[13], line 257\u001b[0m, in \u001b[0;36mload_or_create_data\u001b[1;34m(subreddits, posts_per_subreddit)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Load and update train cache\u001b[39;00m\n\u001b[0;32m    256\u001b[0m train_cache_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_reddit_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 257\u001b[0m X_train_new, y_train_new, df_train_new \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_reddit_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_posts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_cache_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Load and update test cache\u001b[39;00m\n\u001b[0;32m    260\u001b[0m test_cache_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_reddit_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[13], line 132\u001b[0m, in \u001b[0;36mprocess_reddit_posts\u001b[1;34m(posts, cache_file)\u001b[0m\n\u001b[0;32m    130\u001b[0m input_text \u001b[38;5;241m=\u001b[39m post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     json_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral:7b-instruct-v0.3-q4_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInvestmentPostTopicClassification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     result \u001b[38;5;241m=\u001b[39m json_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mparsed\n\u001b[0;32m    142\u001b[0m     InvestmentPostTopicClassification\u001b[38;5;241m.\u001b[39mmodel_validate(result)\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py:158\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[1;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[0;32m    153\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[0;32m    154\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[0;32m    155\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1238\u001b[0m     )\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m    967\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    975\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\cha\\nnfolder\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subreddits = [\"CryptoCurrency\", \"CryptoMoonShots\", \"SatoshiStreetBets\", \"CryptoMarkets\", \"altcoin\"]\n",
    "    posts_per_subreddit = 150  # Increased to get more posts\n",
    "\n",
    "    # Load or create data, appending new posts\n",
    "    X_train, y_train, X_test, y_test, df_train, df_test = load_or_create_data(subreddits, posts_per_subreddit)\n",
    "\n",
    "    # Reshape y_train and y_test to 1D if necessary\n",
    "    if len(y_train.shape) > 1:\n",
    "        y_train = y_train.flatten()\n",
    "        y_test = y_test.flatten()\n",
    "        print(\"Reshaped y_train and y_test to 1D arrays\")\n",
    "\n",
    "    # Print data summary and shapes\n",
    "    print(\"Train sentiment mean:\", np.mean(y_train))\n",
    "    print(\"Test sentiment mean:\", np.mean(y_test))\n",
    "    print(f\"Total posts processed: {len(df_train) + len(df_test)}\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    # Hyperparameter tuning configurations (unchanged)\n",
    "    architectures = [\n",
    "        [Layer_Dense(768, 32, weight_regularizer_l2=5e-3), Activation_ReLU(), Layer_Dropout(0.5), Layer_Dense(32, 1, weight_regularizer_l2=5e-3), Activation_Linear()],\n",
    "        [Layer_Dense(768, 64, weight_regularizer_l2=5e-3), Activation_ReLU(), Layer_Dropout(0.2), Layer_Dense(64, 32, weight_regularizer_l2=5e-3), Activation_ReLU(), Layer_Dense(32, 1, weight_regularizer_l2=5e-3), Activation_Linear()],\n",
    "        [Layer_Dense(768, 128, weight_regularizer_l2=5e-3), Activation_ReLU(), Layer_Dropout(0.4), Layer_Dense(128, 1, weight_regularizer_l2=5e-3), Activation_Linear()]\n",
    "    ]\n",
    "    learning_rates = [0.0005, 0.0001]\n",
    "    batch_sizes = [4, 8]\n",
    "    l2_values = [5e-3, 1e-2]\n",
    "    optimizers = [\n",
    "        Optimizer_SGD(learning_rate=0.0005, decay=1e-5, momentum=0.9, clipnorm=0.5),\n",
    "        Optimizer_Adam(learning_rate=0.0005, decay=1e-5, clipnorm=0.5)\n",
    "    ]\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    best_model = None\n",
    "    best_history = None\n",
    "\n",
    "    for arch_idx, layers in enumerate(architectures):\n",
    "        for lr in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                for l2 in l2_values:\n",
    "                    for opt_idx, optimizer in enumerate(optimizers):\n",
    "                        optimizer.learning_rate = lr\n",
    "                        optimizer.current_learning_rate = lr\n",
    "                        print(f\"\\nTesting config: Arch {arch_idx}, LR={lr}, Batch Size={batch_size}, L2={l2}, Optimizer={'SGD' if opt_idx == 0 else 'Adam'}\")\n",
    "                        model = Model()\n",
    "                        for layer in layers:\n",
    "                            if isinstance(layer, Layer_Dense):\n",
    "                                layer.weight_regularizer_l2 = l2\n",
    "                            model.add(layer)\n",
    "                        model.set(\n",
    "                            loss=Loss_MeanSquaredError(),\n",
    "                            optimizer=optimizer,\n",
    "                            accuracy=Accuracy_Regression(tolerance=0.5)\n",
    "                        )\n",
    "                        model.finalize()\n",
    "\n",
    "                        # Train with early stopping\n",
    "                        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "                        best_epoch_loss = float('inf')\n",
    "                        patience = 10\n",
    "                        patience_counter = 0\n",
    "                        for epoch in range(100):\n",
    "                            model.train(X_train, y_train, epochs=1, batch_size=batch_size, print_every=1000, validation_data=(X_test, y_test))\n",
    "                            train_loss = np.mean(model.loss.forward(model.forward(X_train, training=False), y_train))\n",
    "                            val_loss = np.mean(model.loss.forward(model.forward(X_test, training=False), y_test))\n",
    "                            train_acc = model.accuracy.calculate(model.forward(X_train, training=False), y_train)\n",
    "                            val_acc = model.accuracy.calculate(model.forward(X_test, training=False), y_test)\n",
    "                            history['train_loss'].append(train_loss)\n",
    "                            history['val_loss'].append(val_loss)\n",
    "                            history['train_acc'].append(train_acc)\n",
    "                            history['val_acc'].append(val_acc)\n",
    "                            print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                            if np.isnan(train_loss) or np.isnan(val_loss):\n",
    "                                print(\"NaN detected, stopping this configuration\")\n",
    "                                break\n",
    "                            if val_loss < best_epoch_loss:\n",
    "                                best_epoch_loss = val_loss\n",
    "                                patience_counter = 0\n",
    "                            else:\n",
    "                                patience_counter += 1\n",
    "                            if patience_counter >= patience:\n",
    "                                print(\"Early stopping triggered\")\n",
    "                                break\n",
    "                        if not np.isnan(best_epoch_loss) and best_epoch_loss < best_val_loss:\n",
    "                            best_val_loss = best_epoch_loss\n",
    "                            best_config = (arch_idx, lr, batch_size, l2, 'SGD' if opt_idx == 0 else 'Adam')\n",
    "                            best_model = model\n",
    "                            best_history = history\n",
    "                            best_model.save(\"BabyLlama.pkl\")\n",
    "\n",
    "    # Print best configuration\n",
    "    print(\"\\nBest Configuration:\")\n",
    "    print(f\"Architecture: {best_config[0]}, LR: {best_config[1]}, Batch Size: {best_config[2]}, L2: {best_config[3]}, Optimizer: {best_config[4]}\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Predictions with best model\n",
    "    if best_model is not None:\n",
    "        predictions = best_model.predict(X_test)\n",
    "        df_test['predicted_sentiment'] = predictions.flatten()\n",
    "\n",
    "        print(\"\\nTest DataFrame with Predictions:\")\n",
    "        print(df_test[['id', 'subreddit', 'title', 'sentiment_score', 'predicted_sentiment']].to_string(index=False))\n",
    "\n",
    "        # Plotting results\n",
    "        import matplotlib.pyplot as plt\n",
    "        epochs_range = range(1, len(best_history['train_loss']) + 1)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, best_history['train_loss'], label='Training Loss')\n",
    "        plt.plot(epochs_range, best_history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, best_history['train_acc'], label='Training Accuracy')\n",
    "        plt.plot(epochs_range, best_history['val_acc'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Example: Check structured output for a specific post ID\n",
    "    if not df_test.empty:\n",
    "        example_post_id = df_test['id'].iloc[0]\n",
    "        get_result_by_post_id(df_test, example_post_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
